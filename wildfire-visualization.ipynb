{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5dc87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap (for Colab / fresh environments)\n",
    "#\n",
    "# This cell makes the notebook runnable even when you upload it by itself:\n",
    "# - Clones the GitHub repo if it is not already present in the current working directory.\n",
    "# - Changes into the repo directory so relative paths (e.g., `data/...`) work.\n",
    "# - Adds the repo root to `sys.path` so `import simulation` works.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "REPO_URL = \"https://github.com/marilevay/wildfire-simulation.git\"\n",
    "REPO_DIRNAME = \"wildfire-simulation\"\n",
    "\n",
    "cwd = Path().resolve()\n",
    "repo_dir = cwd / REPO_DIRNAME\n",
    "\n",
    "# If the notebook is run *inside* the repo already, keep that.\n",
    "if (cwd / \"simulation\").exists():\n",
    "    repo_dir = cwd\n",
    "\n",
    "if not repo_dir.exists():\n",
    "    print(f\"Cloning repo into {repo_dir} ...\")\n",
    "    subprocess.check_call([\"git\", \"clone\", REPO_URL, str(repo_dir)])\n",
    "\n",
    "os.chdir(repo_dir)\n",
    "\n",
    "# Ensure imports resolve from the cloned repo.\n",
    "if str(repo_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_dir))\n",
    "\n",
    "print(\"Repo ready at:\", repo_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2192889",
   "metadata": {},
   "source": [
    "# Main Wildfire Visualizations\n",
    "\n",
    "This notebook performs the end-to-end workflow for the wildfire simulation project:\n",
    "\n",
    "- Download Hansen Global Forest Change rasters for a single 10°×10° tile.\n",
    "- Crop + downsample the rasters into a tractable simulation grid.\n",
    "- Construct a simple annual fuel-density time series (2000–2019) from `treecover2000`, `lossyear`, `gain`, and `datamask`.\n",
    "- Run Monte Carlo wildfire simulations per year under two scenarios:\n",
    "  - **Baseline** (no mitigation)\n",
    "  - **Thinning** (uniform fuel reduction)\n",
    "- Visualize results (risk maps and time-series summaries).\n",
    "- Quantify thinning effectiveness with confidence intervals and a permutation-test p-value with a Holm–Bonferroni correction across years.\n",
    "\n",
    "Notes:\n",
    "- The selected tile is **60N_120W** (wilderness of northern Canada).\n",
    "- Many cells below can take time to run (especially Monte Carlo loops); keep `n_runs` modest while iterating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671f4297",
   "metadata": {},
   "source": [
    "# Imports\n",
    "\n",
    "These imports cover:\n",
    "- **Data acquisition**: `requests`, `PIL.Image`, `Path`\n",
    "- **Computation**: `numpy`\n",
    "- **Plotting**: `matplotlib`\n",
    "- **Progress bars**: `tqdm` for long-running loops\n",
    "\n",
    "Keeping imports near the top makes the notebook easier to rerun from a fresh kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2575c772",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import requests\n",
    "import io\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf4832c",
   "metadata": {},
   "source": [
    "## Sanity checks (quick)\n",
    "\n",
    "These checks are a lightweight replacement for the separate `tests.ipynb` notebook.\n",
    "\n",
    "**Goal:** catch breaking changes in the `simulation/` package before running the slower visualization/Monte Carlo sections.\n",
    "\n",
    "**What we verify:**\n",
    "- `Forest` clamps densities into `[0, 1]` and preserves shape.\n",
    "- `Forest.apply_thinning(factor)` returns a new `Forest`, preserves shape, and never increases density.\n",
    "- `MonteCarlo.run()` returns burned/affected fractions in `[0, 1]` with `affected ≥ burned`.\n",
    "- `MonteCarlo.risk_map(n_runs)` returns a grid-shaped probability map in `[0, 1]`.\n",
    "\n",
    "If these pass, the rest of the notebook should run assuming reasonable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d3465c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check 1: imports and basic package wiring\n",
    "#\n",
    "# Specification:\n",
    "# - `import simulation` should succeed after the bootstrap cell.\n",
    "# - Core symbols should import without side effects.\n",
    "\n",
    "from simulation import DataCollector, FireModelParams, Forest, MonteCarlo\n",
    "\n",
    "print(\"Imported simulation OK\")\n",
    "\n",
    "# Sanity check 2: Forest clamps density to [0, 1] and preserves shape\n",
    "\n",
    "density = np.array([[1.2, -0.1], [0.4, 0.0]], dtype=float)\n",
    "forest = Forest(density)\n",
    "\n",
    "assert forest.density.shape == (2, 2), \"Forest should preserve density shape\"\n",
    "assert float(forest.density.max()) <= 1.0, \"Forest should clamp density upper bound to 1\"\n",
    "assert float(forest.density.min()) >= 0.0, \"Forest should clamp density lower bound to 0\"\n",
    "\n",
    "print(\"Forest init/clipping OK\")\n",
    "\n",
    "# Sanity check 3: thinning reduces density and preserves shape\n",
    "\n",
    "thin_factor = 0.5\n",
    "thin = forest.apply_thinning(thin_factor)\n",
    "\n",
    "assert thin is not forest, \"apply_thinning should return a new Forest\"\n",
    "assert thin.density.shape == forest.density.shape, \"Thinned forest should preserve shape\"\n",
    "assert np.all(thin.density <= forest.density + 1e-12), \"Thinning should not increase density\"\n",
    "\n",
    "print(\"Forest thinning OK\")\n",
    "\n",
    "# Sanity check 4: Monte Carlo outputs are well-formed\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "density3 = np.clip(rng.random((30, 30)) * 0.8, 0.0, 1.0)\n",
    "forest3 = Forest(density3)\n",
    "\n",
    "params_check = FireModelParams(\n",
    "    wind=(1.0, 0.0),\n",
    "    wind_strength=0.3,\n",
    "    density_exponent=1.2,\n",
    "    base_spread=0.7,\n",
    ")\n",
    "\n",
    "n_runs_check = 25\n",
    "mc_check = MonteCarlo(forest=forest3, params=params_check, n_runs=n_runs_check, rng=np.random.default_rng(1))\n",
    "collector_check = mc_check.run()\n",
    "burned_check, affected_check = collector_check.convert_to_arrays()\n",
    "\n",
    "assert burned_check.shape == (n_runs_check,), \"burned_fraction should have length n_runs\"\n",
    "assert affected_check.shape == (n_runs_check,), \"affected_fraction should have length n_runs\"\n",
    "assert np.all((burned_check >= 0.0) & (burned_check <= 1.0)), \"burned fractions must be in [0, 1]\"\n",
    "assert np.all((affected_check >= 0.0) & (affected_check <= 1.0)), \"affected fractions must be in [0, 1]\"\n",
    "assert np.all(affected_check + 1e-12 >= burned_check), \"affected should be >= burned\"\n",
    "\n",
    "risk_check = mc_check.risk_map(n_runs=n_runs_check)\n",
    "assert risk_check.shape == forest3.density.shape, \"risk_map must match forest grid shape\"\n",
    "assert np.all((risk_check >= 0.0) & (risk_check <= 1.0)), \"risk_map values must be in [0, 1]\"\n",
    "\n",
    "mean, lo, hi = DataCollector.calculate_ci95_mean(burned_check)\n",
    "assert lo <= mean <= hi, \"CI bounds must satisfy lo <= mean <= hi\"\n",
    "\n",
    "print(\"MonteCarlo + risk_map + CI helper OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec6cba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the URL of the data tile\n",
    "\n",
    "# This project uses the Hansen GFC 2019 v1.7 dataset, which is distributed as 10°×10° GeoTIFF tiles.\n",
    "# The tile identifier is embedded in the filename (e.g., `60N_120W`).\n",
    "\n",
    "# You can swap this URL to analyze a different tile.\n",
    "url = (\n",
    "    \"https://storage.googleapis.com/earthenginepartners-hansen/GFC-2019-v1.7/\"\n",
    "    \"Hansen_GFC-2019-v1.7_treecover2000_60N_120W.tif\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a7840a",
   "metadata": {},
   "source": [
    "## Hansen Forest Tile Download\n",
    "\n",
    "We have to specify the URL of the data tile we want to download.\n",
    "The important part of the URL is the `00N_010E` part at the end.\n",
    "This specifies the latitude and longitude (in 10-degree units)\n",
    "of the data tile to download.\n",
    "\n",
    "The selected data tile is at `60N_120W`, or the wilderness of northern Canada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626ca3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data. These tiles are large, so we cache to disk and only convert the simulation window\n",
    "# to a NumPy array (instead of materializing the full 40,000 x 40,000 raster in memory).\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "def download_if_needed(url: str, out_path: str) -> str:\n",
    "    p = Path(out_path)\n",
    "    p.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if p.exists() and p.stat().st_size > 0:\n",
    "        return str(p)\n",
    "\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(p, \"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=1024 * 1024):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "    return str(p)\n",
    "\n",
    "\n",
    "def load_tif_window(path: str, row0: int, col0: int, window_px: int) -> np.ndarray:\n",
    "    im = Image.open(path)\n",
    "    box = (int(col0), int(row0), int(col0 + window_px), int(row0 + window_px))\n",
    "    return np.array(im.crop(box))\n",
    "\n",
    "\n",
    "treecover_path = download_if_needed(url, \"data/treecover2000_tile.tif\")\n",
    "\n",
    "# Use PIL to get raster dimensions without loading the full array.\n",
    "with Image.open(treecover_path) as _im:\n",
    "    tile_shape = (_im.height, _im.width)\n",
    "\n",
    "tile_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e5439b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: tile dimensions (should be 40000x40000)\n",
    "tile_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e79a789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a small part of the whole data tile (from disk-cached file).\n",
    "preview = load_tif_window(treecover_path, row0=2000, col0=1000, window_px=2000)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(preview, cmap='gray')\n",
    "plt.colorbar(shrink=0.8)\n",
    "plt.title(\"Raw Hansen treecover2000 (subset), values 0..100\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320b39e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the simulation package\n",
    "#\n",
    "# Specification:\n",
    "# - The `simulation/` package lives in this repository, so we add the repository root to `sys.path`\n",
    "#   to make `import simulation` work when running the notebook.\n",
    "\n",
    "# Note: If you install this project as a package instead, you can remove the sys.path manipulation.\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Make the project root importable so import simulation works\n",
    "sys.path.append(str(Path().resolve()))\n",
    "\n",
    "from simulation import FireModelParams, Forest, MonteCarlo, DataCollector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64245ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: crop + downsample Hansen rasters into a simulation-ready grid\n",
    "#\n",
    "# To make the notebook runnable in real time (e.g., during a demo), we support a lightweight cache:\n",
    "# - If a preprocessed `.npz` file exists in `data/`, we load it immediately (fast path).\n",
    "# - Otherwise we download/crop/downsample the raw GeoTIFFs and then save the `.npz` for reuse.\n",
    "\n",
    "def mean_pool_2d(a: np.ndarray, out_shape: tuple[int, int]) -> np.ndarray:\n",
    "    a = np.asarray(a)\n",
    "    in_rows, in_cols = a.shape\n",
    "    out_rows, out_cols = out_shape\n",
    "\n",
    "    if in_rows % out_rows != 0 or in_cols % out_cols != 0:\n",
    "        raise ValueError(\"For this simple pooling method, crop so window dims are divisible by out dims\")\n",
    "\n",
    "    r = in_rows // out_rows\n",
    "    c = in_cols // out_cols\n",
    "    return a.reshape(out_rows, r, out_cols, c).mean(axis=(1, 3))\n",
    "\n",
    "\n",
    "# Select a window from the tile and downsample it to a simulation grid.\n",
    "row0, col0 = 2000, 1000\n",
    "window_px = 2000\n",
    "sim_shape = (200, 200)\n",
    "\n",
    "# Identify the tile and construct a deterministic cache filename.\n",
    "tile_id = url.split(\"_treecover2000_\")[-1].replace(\".tif\", \"\")\n",
    "preprocessed_path = Path(f\"data/preprocessed_{tile_id}_r{row0}_c{col0}_w{window_px}_s{sim_shape[0]}x{sim_shape[1]}.npz\")\n",
    "preprocessed_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if preprocessed_path.exists():\n",
    "    loaded = np.load(preprocessed_path)\n",
    "\n",
    "    base_density_2000 = loaded[\"base_density_2000\"]\n",
    "    land_small = loaded[\"land_small\"]\n",
    "    water_small = loaded[\"water_small\"]\n",
    "    nodata_small = loaded[\"nodata_small\"]\n",
    "    lossyear_small = loaded[\"lossyear_small\"]\n",
    "    gain_small = loaded[\"gain_small\"]\n",
    "\n",
    "    print(f\"Loaded preprocessed arrays from {preprocessed_path}\")\n",
    "else:\n",
    "    # Derive matching layer URLs from the treecover URL.\n",
    "    base = \"https://storage.googleapis.com/earthenginepartners-hansen/GFC-2019-v1.7/\"\n",
    "\n",
    "    datamask_url = base + f\"Hansen_GFC-2019-v1.7_datamask_{tile_id}.tif\"\n",
    "    lossyear_url = base + f\"Hansen_GFC-2019-v1.7_lossyear_{tile_id}.tif\"\n",
    "    gain_url = base + f\"Hansen_GFC-2019-v1.7_gain_{tile_id}.tif\"\n",
    "\n",
    "    datamask_path = download_if_needed(datamask_url, \"data/datamask_tile.tif\")\n",
    "    lossyear_path = download_if_needed(lossyear_url, \"data/lossyear_tile.tif\")\n",
    "    gain_path = download_if_needed(gain_url, \"data/gain_tile.tif\")\n",
    "\n",
    "    # Load raw windows\n",
    "    window_tree = load_tif_window(treecover_path, row0=row0, col0=col0, window_px=window_px)\n",
    "    window_mask = load_tif_window(datamask_path, row0=row0, col0=col0, window_px=window_px)\n",
    "    window_loss = load_tif_window(lossyear_path, row0=row0, col0=col0, window_px=window_px)\n",
    "    window_gain = load_tif_window(gain_path, row0=row0, col0=col0, window_px=window_px)\n",
    "\n",
    "    # Crop to be divisible for pooling\n",
    "    h = sim_shape[0] * (window_tree.shape[0] // sim_shape[0])\n",
    "    w = sim_shape[1] * (window_tree.shape[1] // sim_shape[1])\n",
    "    window_tree = window_tree[:h, :w]\n",
    "    window_mask = window_mask[:h, :w]\n",
    "    window_loss = window_loss[:h, :w]\n",
    "    window_gain = window_gain[:h, :w]\n",
    "\n",
    "    # Downsample masks\n",
    "    mask_small = mean_pool_2d(window_mask.astype(float), sim_shape)\n",
    "    # datamask values are categorical; after pooling, interpret >1.5 as \"mostly water\" and <0.5 as \"mostly no data\".\n",
    "    water_small = mask_small > 1.5\n",
    "    nodata_small = mask_small < 0.5\n",
    "    land_small = (~water_small) & (~nodata_small)\n",
    "\n",
    "    # Downsample and normalize density\n",
    "    base_density_2000 = np.clip(mean_pool_2d(window_tree.astype(float), sim_shape) / 100.0, 0.0, 1.0)\n",
    "    base_density_2000 = base_density_2000 * land_small.astype(float)\n",
    "\n",
    "    # lossyear: values are 0..19; pooling yields floats, so we round to nearest int\n",
    "    lossyear_small = mean_pool_2d(window_loss.astype(float), sim_shape)\n",
    "    lossyear_small = np.rint(lossyear_small).astype(int)\n",
    "\n",
    "    # gain is 0/1, but pooling yields fractions. Threshold at 0.5\n",
    "    # (meaning >50% of the pooled pixels gained)\n",
    "    gain_small = mean_pool_2d(window_gain.astype(float), sim_shape) > 0.5\n",
    "\n",
    "    # Save a lightweight cache so future runs don't require downloading huge rasters.\n",
    "    np.savez_compressed(\n",
    "        preprocessed_path,\n",
    "        base_density_2000=base_density_2000,\n",
    "        land_small=land_small,\n",
    "        water_small=water_small,\n",
    "        nodata_small=nodata_small,\n",
    "        lossyear_small=lossyear_small,\n",
    "        gain_small=gain_small,\n",
    "    )\n",
    "    print(f\"Saved preprocessed arrays to {preprocessed_path}\")\n",
    "\n",
    "\n",
    "# Quick visual sanity checks\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(base_density_2000, cmap=\"Greens\")\n",
    "plt.colorbar(shrink=0.8)\n",
    "plt.title(f\"Density (year 2000) with datamask applied {base_density_2000.shape}\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(water_small.astype(int), cmap=\"Blues\")\n",
    "plt.title(\"Water mask (1=water)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a229649",
   "metadata": {},
   "source": [
    "## Baseline vs Thinning Analysis\n",
    "We compare a baseline (no mitigation) scenario to a simple mitigation strategy:\n",
    "\n",
    "- **Thinning (fuel reduction):** multiply the forest density everywhere by a constant factor `c < 1`\n",
    "  (here `thin_factor = 0.6`).\n",
    "\n",
    "Lower density reduces spread probability and can disconnect fuel pathways, reducing expected burned\n",
    "and affected fractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c447504f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permutation test helpers (p-values) + Holm–Bonferroni correction\n",
    "#\n",
    "# We quantify thinning effectiveness with a nonparametric permutation test:\n",
    "# - Statistic: difference in means, T = mean(baseline) - mean(thinning)\n",
    "# - Alternative: thinning reduces outcomes, so we test H1: E[baseline] > E[thinning]\n",
    "#\n",
    "# We then correct across multiple years using Holm–Bonferroni to control family-wise error.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def permutation_p_value(x, y, n_perm=5000, alternative=\"greater\", rng=None):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng(0)\n",
    "\n",
    "    t_obs = x.mean() - y.mean()\n",
    "    pooled = np.concatenate([x, y])\n",
    "    n_x = len(x)\n",
    "\n",
    "    count = 0\n",
    "    for _ in range(int(n_perm)):\n",
    "        perm = rng.permutation(pooled)\n",
    "        x_p = perm[:n_x]\n",
    "        y_p = perm[n_x:]\n",
    "        t = x_p.mean() - y_p.mean()\n",
    "\n",
    "        if alternative == \"greater\":\n",
    "            count += (t >= t_obs)\n",
    "        elif alternative == \"less\":\n",
    "            count += (t <= t_obs)\n",
    "        else:\n",
    "            count += (abs(t) >= abs(t_obs))\n",
    "\n",
    "    return (count + 1) / (n_perm + 1), float(t_obs)\n",
    "\n",
    "\n",
    "def holm_bonferroni(p_values, alpha=0.05):\n",
    "    \"\"\"Holm–Bonferroni multiple comparisons correction.\n",
    "\n",
    "    Returns:\n",
    "      - adjusted_p: Holm adjusted p-values (same order as input)\n",
    "      - reject: whether each hypothesis is rejected at family-wise error rate alpha\n",
    "    \"\"\"\n",
    "    p = np.asarray(p_values, dtype=float)\n",
    "    m = len(p)\n",
    "    order = np.argsort(p)\n",
    "    p_sorted = p[order]\n",
    "\n",
    "    # Holm adjusted p-values: adj_i = max_{j<=i} ((m-j)*p_(j)) with j 0-indexed\n",
    "    adj_sorted = np.empty(m, dtype=float)\n",
    "    running_max = 0.0\n",
    "    for i, p_i in enumerate(p_sorted):\n",
    "        adj_i = (m - i) * p_i\n",
    "        running_max = max(running_max, adj_i)\n",
    "        adj_sorted[i] = min(1.0, running_max)\n",
    "\n",
    "    adjusted_p = np.empty(m, dtype=float)\n",
    "    adjusted_p[order] = adj_sorted\n",
    "\n",
    "    # Rejection decisions (step-down)\n",
    "    reject_sorted = np.zeros(m, dtype=bool)\n",
    "    for i, p_i in enumerate(p_sorted):\n",
    "        if p_i <= alpha / (m - i):\n",
    "            reject_sorted[i] = True\n",
    "        else:\n",
    "            break\n",
    "    reject = np.empty(m, dtype=bool)\n",
    "    reject[order] = reject_sorted\n",
    "\n",
    "    return adjusted_p, reject"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917a8765",
   "metadata": {},
   "source": [
    "# Risk Map time series \n",
    "From 2000 to 2019 using additional lossyear and gain masks \n",
    "\n",
    "**Assumptions:**\n",
    "- If a cell has loss in year Y (lossyear == Y-2000), then from that year onward the density is reduced.\n",
    "- gain is treated as a modest regrowth applied after 2012 in cells marked as gain.\n",
    "- water/no-data cells (datamask) always have density 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4cdb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a simple density time series (2000..2019) using lossyear and gain.\n",
    "\n",
    "p_burned, p_affected = [], []\n",
    "delta_burned, delta_affected = [], []\n",
    "\n",
    "def density_for_year(year: int) -> np.ndarray:\n",
    "    if year < 2000 or year > 2019:\n",
    "        raise ValueError(\"year must be in [2000, 2019]\")\n",
    "\n",
    "    d = base_density_2000.copy()\n",
    "\n",
    "    # Apply loss: lossyear values are 1..19 representing 2001..2019.\n",
    "    # For a given simulation year, treat any loss event up to that year as removing forest density.\n",
    "    cutoff = year - 2000\n",
    "    lost = (lossyear_small > 0) & (lossyear_small <= cutoff)\n",
    "\n",
    "    # Simple loss model: set to zero (stand-replacing disturbance)\n",
    "    d[lost] = 0.0\n",
    "\n",
    "    # Apply gain: only after 2012, modestly increase density in gain cells.\n",
    "    if year >= 2012:\n",
    "        d[gain_small] = np.clip(d[gain_small] + 0.25, 0.0, 1.0)\n",
    "\n",
    "    # Keep land mask enforced\n",
    "    d *= land_small.astype(float)\n",
    "    return d\n",
    "\n",
    "\n",
    "# Monte Carlo over selected years + a strategy comparison (baseline vs thinning)\n",
    "params = FireModelParams(\n",
    "    wind=(1.0, 0.0),\n",
    "    wind_strength=0.4,\n",
    "    density_exponent=1.2,\n",
    "    base_spread=0.7,\n",
    ")\n",
    "\n",
    "n_runs = 200\n",
    "\n",
    "# Include more years to show intermediate changes.\n",
    "years_to_analyze = (2000, 2003, 2005, 2008, 2010, 2012, 2015, 2017, 2019)\n",
    "years_to_plot = years_to_analyze  # Alias for compatibility with other cells\n",
    "\n",
    "results = {}\n",
    "for y in tqdm(years_to_analyze, desc=\"Years\"):\n",
    "    d = density_for_year(y)\n",
    "    forest = Forest(d)\n",
    "    mc = MonteCarlo(forest=forest, params=params, n_runs=n_runs, rng=np.random.default_rng(y))\n",
    "\n",
    "    report = mc.run()\n",
    "    burned_arr, affected_arr = report.convert_to_arrays()\n",
    "    risk = mc.risk_map(n_runs=n_runs)\n",
    "    results[(y, \"baseline\")] = (burned_arr, affected_arr, risk)\n",
    "\n",
    "    burned_mean, burned_lo, burned_hi = DataCollector.calculate_ci95_mean(burned_arr)\n",
    "    affected_mean, affected_lo, affected_hi = DataCollector.calculate_ci95_mean(affected_arr)\n",
    "    print(f\"Year {y} baseline burned mean+CI:\", (burned_mean, burned_lo, burned_hi))\n",
    "    print(f\"Year {y} baseline affected mean+CI:\", (affected_mean, affected_lo, affected_hi))\n",
    "\n",
    "    # Strategy: thinning\n",
    "    thin_factor = 0.6\n",
    "    forest_thin = forest.apply_thinning(thin_factor)\n",
    "    mc_thin = MonteCarlo(forest=forest_thin, params=params, n_runs=n_runs, rng=np.random.default_rng(y + 1))\n",
    "\n",
    "    report_thin = mc_thin.run()\n",
    "    burned_arr_t, affected_arr_t = report_thin.convert_to_arrays()\n",
    "    risk_thin = mc_thin.risk_map(n_runs=n_runs)\n",
    "    results[(y, \"thinning\")] = (burned_arr_t, affected_arr_t, risk_thin)\n",
    "\n",
    "    # Store p-values and effect sizes for later correction\n",
    "    # (define these lists once before the loop)\n",
    "    # p_burned = []; p_affected = []; delta_burned = []; delta_affected = []\n",
    "\n",
    "    p_b, d_b = permutation_p_value(burned_arr, burned_arr_t, n_perm=5000, alternative=\"greater\",\n",
    "                                rng=np.random.default_rng(1000 + y))\n",
    "    p_a, d_a = permutation_p_value(affected_arr, affected_arr_t, n_perm=5000, alternative=\"greater\",\n",
    "                                rng=np.random.default_rng(2000 + y))\n",
    "\n",
    "    p_burned.append(p_b); delta_burned.append(d_b)\n",
    "    p_affected.append(p_a); delta_affected.append(d_a)\n",
    "\n",
    "    print(f\"Year {y}: Delta burned={d_b:.4f}, p_raw_burned={p_b:.4g} | Delta affected={d_a:.4f}, p_raw_affected={p_a:.4g}\")\n",
    "\n",
    "    burned_mean_t, burned_lo_t, burned_hi_t = DataCollector.calculate_ci95_mean(burned_arr_t)\n",
    "    affected_mean_t, affected_lo_t, affected_hi_t = DataCollector.calculate_ci95_mean(affected_arr_t)\n",
    "    print(f\"Year {y} thinning burned mean+CI:\", (burned_mean_t, burned_lo_t, burned_hi_t))\n",
    "    print(f\"Year {y} thinning affected mean+CI:\", (affected_mean_t, affected_lo_t, affected_hi_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49d681f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all density maps together (row or grid)\n",
    "#\n",
    "# Uses `years_to_plot` + `density_for_year` defined above.\n",
    "# Set `layout` to \"row\" to put everything in one horizontal line.\n",
    "\n",
    "import math\n",
    "\n",
    "layout = \"grid\"  # \"grid\" or \"row\"\n",
    "\n",
    "maps = [density_for_year(y) for y in years_to_plot]\n",
    "\n",
    "# Use a consistent scale across years so colors are comparable.\n",
    "vmin = 0.0\n",
    "vmax = 1.0\n",
    "\n",
    "n = len(years_to_plot)\n",
    "if layout == \"row\":\n",
    "    ncols = n\n",
    "else:\n",
    "    ncols = 3  # change to 4 if you want a wider grid\n",
    "nrows = int(math.ceil(n / ncols))\n",
    "\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(4 * ncols, 4 * nrows), constrained_layout=True)\n",
    "axes = np.atleast_1d(axes).ravel()\n",
    "\n",
    "im = None\n",
    "for i, (y, d) in enumerate(zip(years_to_plot, maps)):\n",
    "    ax = axes[i]\n",
    "    im = ax.imshow(d, cmap=\"Greens\", vmin=vmin, vmax=vmax)\n",
    "    ax.set_title(f\"{y}\")\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "# Turn off any unused subplot slots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].axis(\"off\")\n",
    "\n",
    "# One shared colorbar\n",
    "fig.colorbar(im, ax=axes[:n], shrink=0.8, label=\"density\")\n",
    "fig.suptitle(\"Forest density maps across years\", y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b957d643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risk map summaries over time (convert 2D risk maps into time-series lines)\n",
    "#\n",
    "# A risk map is a 2D field of per-cell probabilities. To plot \"risk\" as a line over time, we reduce\n",
    "# each map to scalar summaries:\n",
    "# - mean risk across the grid\n",
    "# - upper-tail quantiles (p90, p99) to track hotspot intensity\n",
    "# - fraction of cells above a threshold (e.g., risk > 0.5) to track high-risk area\n",
    "\n",
    "years = list(years_to_analyze)\n",
    "\n",
    "def risk_summaries(risk, threshold=0.5):\n",
    "    r = np.asarray(risk, float).ravel()\n",
    "    return {\n",
    "        \"mean\": float(r.mean()),\n",
    "        \"p90\": float(np.quantile(r, 0.90)),\n",
    "        \"p99\": float(np.quantile(r, 0.99)),\n",
    "        \"frac_gt_t\": float((r > threshold).mean()),\n",
    "    }\n",
    "\n",
    "baseline_risk_mean = []\n",
    "thinning_risk_mean = []\n",
    "baseline_risk_p90 = []\n",
    "thinning_risk_p90 = []\n",
    "baseline_risk_frac = []\n",
    "thinning_risk_frac = []\n",
    "\n",
    "thr = 0.5\n",
    "\n",
    "for y in years:\n",
    "    _, _, risk_b = results[(y, \"baseline\")]\n",
    "    _, _, risk_t = results[(y, \"thinning\")]\n",
    "\n",
    "    sb = risk_summaries(risk_b, threshold=thr)\n",
    "    st = risk_summaries(risk_t, threshold=thr)\n",
    "\n",
    "    baseline_risk_mean.append(sb[\"mean\"])\n",
    "    thinning_risk_mean.append(st[\"mean\"])\n",
    "    baseline_risk_p90.append(sb[\"p90\"])\n",
    "    thinning_risk_p90.append(st[\"p90\"])\n",
    "    baseline_risk_frac.append(sb[\"frac_gt_t\"])\n",
    "    thinning_risk_frac.append(st[\"frac_gt_t\"])\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(years, baseline_risk_mean, marker=\"o\", label=\"baseline\")\n",
    "plt.plot(years, thinning_risk_mean, marker=\"o\", label=\"thinning\")\n",
    "plt.title(\"Mean risk over time\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Mean Risk\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(years, baseline_risk_p90, marker=\"o\", label=\"baseline\")\n",
    "plt.plot(years, thinning_risk_p90, marker=\"o\", label=\"thinning\")\n",
    "plt.title(\"90th percentile risk over time (hotspot intensity)\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Risk (90th percentile)\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(years, baseline_risk_frac, marker=\"o\", label=\"baseline\")\n",
    "plt.plot(years, thinning_risk_frac, marker=\"o\", label=\"thinning\")\n",
    "plt.title(f\"High-risk area over time (risk > {thr})\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Fraction of Cells\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2ce15b",
   "metadata": {},
   "source": [
    "## Burned and Affected Area Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46559f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Burned + affected fractions over time (mean ± 95% CI)\n",
    "#\n",
    "# This is the main time-series result:\n",
    "# - Burned fraction: fraction of cells that burn per run\n",
    "# - Affected fraction: burned + adjacent-to-burned per run\n",
    "#\n",
    "# For each year and scenario (baseline vs thinning), we compute a mean and a normal-approximation 95% CI.\n",
    "\n",
    "years = list(years_to_analyze)\n",
    "\n",
    "def mean_ci(arr):\n",
    "    m, lo, hi = DataCollector.calculate_ci95_mean(np.asarray(arr, float))\n",
    "    return float(m), float(lo), float(hi)\n",
    "\n",
    "burn_mean_b, burn_lo_b, burn_hi_b = [], [], []\n",
    "burn_mean_t, burn_lo_t, burn_hi_t = [], [], []\n",
    "aff_mean_b, aff_lo_b, aff_hi_b = [], [], []\n",
    "aff_mean_t, aff_lo_t, aff_hi_t = [], [], []\n",
    "\n",
    "for y in years:\n",
    "    burned_b, affected_b, _ = results[(y, \"baseline\")]\n",
    "    burned_t, affected_t, _ = results[(y, \"thinning\")]\n",
    "\n",
    "    m, lo, hi = mean_ci(burned_b)\n",
    "    burn_mean_b.append(m); burn_lo_b.append(lo); burn_hi_b.append(hi)\n",
    "\n",
    "    m, lo, hi = mean_ci(burned_t)\n",
    "    burn_mean_t.append(m); burn_lo_t.append(lo); burn_hi_t.append(hi)\n",
    "\n",
    "    m, lo, hi = mean_ci(affected_b)\n",
    "    aff_mean_b.append(m); aff_lo_b.append(lo); aff_hi_b.append(hi)\n",
    "\n",
    "    m, lo, hi = mean_ci(affected_t)\n",
    "    aff_mean_t.append(m); aff_lo_t.append(lo); aff_hi_t.append(hi)\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(years, burn_mean_b, marker=\"o\", label=\"baseline\")\n",
    "plt.fill_between(years, burn_lo_b, burn_hi_b, alpha=0.2)\n",
    "plt.plot(years, burn_mean_t, marker=\"o\", label=\"thinning\")\n",
    "plt.fill_between(years, burn_lo_t, burn_hi_t, alpha=0.2)\n",
    "plt.title(\"Burned fraction over time (mean ± 95% CI)\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Burned Fraction\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(years, aff_mean_b, marker=\"o\", label=\"baseline\")\n",
    "plt.fill_between(years, aff_lo_b, aff_hi_b, alpha=0.2)\n",
    "plt.plot(years, aff_mean_t, marker=\"o\", label=\"thinning\")\n",
    "plt.fill_between(years, aff_lo_t, aff_hi_t, alpha=0.2)\n",
    "plt.title(\"Affected fraction over time (mean ± 95% CI)\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Affected Fraction\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f4d6fd",
   "metadata": {},
   "source": [
    "## Significance Test of Thinning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dfc366",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "\n",
    "p_b_adj, rej_b = holm_bonferroni(p_burned, alpha=alpha)\n",
    "p_a_adj, rej_a = holm_bonferroni(p_affected, alpha=alpha)\n",
    "\n",
    "print(\"\\nMultiple comparisons correction (Holm–Bonferroni, per-metric across years)\\n\")\n",
    "for i, y in enumerate(years_to_analyze):\n",
    "    print(\n",
    "        f\"Year {y}: \"\n",
    "        f\"Delta burned={delta_burned[i]:.4f}, p_adj_burned={p_b_adj[i]:.4g}, reject={bool(rej_b[i])} | \"\n",
    "        f\"Delta affected={delta_affected[i]:.4f}, p_adj_affected={p_a_adj[i]:.4g}, reject={bool(rej_a[i])}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9960e67",
   "metadata": {},
   "source": [
    "# Theoretical vs empirical comparison (density scaling sweeps)\n",
    "This section connects the simulation to percolation/connectivity intuition.\n",
    "\n",
    "Idea:\n",
    "- Scale a fixed year's density map by a factor c in [0, 1] (uniform thinning).\n",
    "- Estimate the expected burned fraction as a function of c.\n",
    "- A nonlinear/transition-like curve supports the connectivity intuition.\n",
    "\n",
    "Minimal robustness setup:\n",
    "- Run the sweep for 3 representative years:\n",
    "  * 2000 (baseline)\n",
    "  * 2012 (gain breakpoint in our simplified time series)\n",
    "  * 2019 (end of available lossyear range)\n",
    "  \n",
    "Note: This is not part of the thinning significance test; it's a qualitative/theory check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179dd472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Density scaling sweep (multi-year overlay)\n",
    "\n",
    "base_years_for_sweep = (2000, 2012, 2019)\n",
    "\n",
    "# Keep modest so runtime stays reasonable.\n",
    "sweep_runs = 100\n",
    "scales = np.linspace(0.1, 1.0, 10)\n",
    "\n",
    "\n",
    "def run_density_scaling_sweep(base_year: int, scales, sweep_runs: int):\n",
    "    \"\"\"Run a uniform thinning sweep on a fixed base-year landscape.\n",
    "\n",
    "    Returns:\n",
    "      means: mean burned fraction at each scale\n",
    "      ci_lo/ci_hi: 95% CI bounds for the mean at each scale\n",
    "    \"\"\"\n",
    "    base_density = density_for_year(base_year)\n",
    "    base_forest = Forest(base_density)\n",
    "\n",
    "    means, ci_lo, ci_hi = [], [], []\n",
    "\n",
    "    for c in scales:\n",
    "        forest_c = base_forest.apply_thinning(float(c))\n",
    "        mc_c = MonteCarlo(\n",
    "            forest=forest_c,\n",
    "            params=params,\n",
    "            n_runs=sweep_runs,\n",
    "            rng=np.random.default_rng(base_year * 10_000 + int(1_000 * c)),\n",
    "        )\n",
    "\n",
    "        report_c = mc_c.run()\n",
    "        burned_c, _ = report_c.convert_to_arrays()\n",
    "        mean_c, lo_c, hi_c = DataCollector.calculate_ci95_mean(burned_c)\n",
    "\n",
    "        means.append(float(mean_c))\n",
    "        ci_lo.append(float(lo_c))\n",
    "        ci_hi.append(float(hi_c))\n",
    "\n",
    "    return means, ci_lo, ci_hi\n",
    "\n",
    "\n",
    "sweep_results = {y: run_density_scaling_sweep(y, scales=scales, sweep_runs=sweep_runs) for y in base_years_for_sweep}\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "for y in base_years_for_sweep:\n",
    "    means, lo, hi = sweep_results[y]\n",
    "    plt.plot(scales, means, marker=\"o\", label=f\"year {y}\")\n",
    "    plt.fill_between(scales, lo, hi, alpha=0.15)\n",
    "\n",
    "plt.title(\"Density scaling sweep (uniform thinning) across representative years\")\n",
    "plt.xlabel(\"Global density scale c (thinning factor)\")\n",
    "plt.ylabel(\"Burned fraction\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897f5202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all density maps together (row or grid)\n",
    "# Uses `years_to_plot` + `density_for_year` defined above.\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "layout = \"grid\"  # \"grid\" or \"row\"\n",
    "\n",
    "maps = [density_for_year(y) for y in years_to_plot]\n",
    "\n",
    "# Keep colors comparable across years\n",
    "vmin, vmax = 0.0, 1.0\n",
    "\n",
    "n = len(years_to_plot)\n",
    "if layout == \"row\":\n",
    "    ncols = n\n",
    "else:\n",
    "    ncols = 3  # change to 4 if you want a wider grid\n",
    "nrows = int(math.ceil(n / ncols))\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=nrows,\n",
    "    ncols=ncols,\n",
    "    figsize=(4 * ncols, 4 * nrows),\n",
    "    constrained_layout=True,\n",
    ")\n",
    "\n",
    "axes = np.atleast_1d(axes).ravel()\n",
    "\n",
    "im = None\n",
    "for i, (y, d) in enumerate(zip(years_to_plot, maps)):\n",
    "    ax = axes[i]\n",
    "    im = ax.imshow(d, cmap=\"Greens\", vmin=vmin, vmax=vmax)\n",
    "    ax.set_title(f\"{y}\")\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "# Turn off unused axes\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].axis(\"off\")\n",
    "\n",
    "fig.colorbar(im, ax=axes[:n], shrink=0.8, label=\"density\")\n",
    "fig.suptitle(\"Forest density maps across years\", y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5000e42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all risk maps together (baseline and thinning) in a grid\n",
    "# This is fast: it only plots arrays already stored in `results`.\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "scenario = \"baseline\"  # \"baseline\" or \"thinning\"\n",
    "layout = \"grid\"        # \"grid\" or \"row\"\n",
    "\n",
    "years = list(years_to_analyze)\n",
    "risk_maps = [results[(y, scenario)][2] for y in years]\n",
    "\n",
    "vmin, vmax = 0.0, 1.0\n",
    "cmap = \"inferno_r\"  # higher risk darker, consistent with your single plots\n",
    "\n",
    "n = len(years)\n",
    "if layout == \"row\":\n",
    "    ncols = n\n",
    "else:\n",
    "    ncols = 3  # try 4 if you want it wider\n",
    "nrows = int(math.ceil(n / ncols))\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=nrows,\n",
    "    ncols=ncols,\n",
    "    figsize=(4 * ncols, 4 * nrows),\n",
    "    constrained_layout=True,\n",
    ")\n",
    "axes = np.atleast_1d(axes).ravel()\n",
    "\n",
    "im = None\n",
    "for i, (y, r) in enumerate(zip(years, risk_maps)):\n",
    "    ax = axes[i]\n",
    "    im = ax.imshow(r, cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "    ax.set_title(f\"{y} ({scenario})\")\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].axis(\"off\")\n",
    "\n",
    "fig.colorbar(im, ax=axes[:n], shrink=0.8, label=\"risk (probability)\")\n",
    "fig.suptitle(f\"Risk maps across years ({scenario})\", y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b354a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all risk maps together (baseline and thinning) in a grid\n",
    "# This is fast: it only plots arrays already stored in `results`.\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "scenario = \"thinning\"  # \"baseline\" or \"thinning\"\n",
    "layout = \"grid\"        # \"grid\" or \"row\"\n",
    "\n",
    "years = list(years_to_analyze)\n",
    "risk_maps = [results[(y, scenario)][2] for y in years]\n",
    "\n",
    "vmin, vmax = 0.0, 1.0\n",
    "cmap = \"inferno_r\"  # higher risk darker, consistent with your single plots\n",
    "\n",
    "n = len(years)\n",
    "if layout == \"row\":\n",
    "    ncols = n\n",
    "else:\n",
    "    ncols = 3  # try 4 if you want it wider\n",
    "nrows = int(math.ceil(n / ncols))\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=nrows,\n",
    "    ncols=ncols,\n",
    "    figsize=(4 * ncols, 4 * nrows),\n",
    "    constrained_layout=True,\n",
    ")\n",
    "axes = np.atleast_1d(axes).ravel()\n",
    "\n",
    "im = None\n",
    "for i, (y, r) in enumerate(zip(years, risk_maps)):\n",
    "    ax = axes[i]\n",
    "    im = ax.imshow(r, cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "    ax.set_title(f\"{y} ({scenario})\")\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].axis(\"off\")\n",
    "\n",
    "fig.colorbar(im, ax=axes[:n], shrink=0.8, label=\"risk (probability)\")\n",
    "fig.suptitle(f\"Risk maps across years ({scenario})\", y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51c55ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Burned fraction histograms together (baseline vs thinning) across years\n",
    "# Fast: uses already-stored arrays in `results`.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "years = list(years_to_analyze)\n",
    "\n",
    "bins = 25\n",
    "xlim = (0.0, 1.0)   # keep consistent across years\n",
    "sharey = True       # makes counts comparable across subplots\n",
    "\n",
    "ncols = 3  # change to 4 if you want a wider grid\n",
    "nrows = int(np.ceil(len(years) / ncols))\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=nrows,\n",
    "    ncols=ncols,\n",
    "    figsize=(4.8 * ncols, 3.6 * nrows),\n",
    "    constrained_layout=True,\n",
    "    sharex=True,\n",
    "    sharey=sharey,\n",
    ")\n",
    "axes = np.atleast_1d(axes).ravel()\n",
    "\n",
    "for i, y in enumerate(years):\n",
    "    burned_b = results[(y, \"baseline\")][0]\n",
    "    burned_t = results[(y, \"thinning\")][0]\n",
    "\n",
    "    ax = axes[i]\n",
    "    ax.hist(burned_b, bins=bins, range=xlim, alpha=0.55, label=\"baseline\")\n",
    "    ax.hist(burned_t, bins=bins, range=xlim, alpha=0.55, label=\"thinning\")\n",
    "    ax.set_title(f\"{y}\")\n",
    "    ax.set_xlim(*xlim)\n",
    "\n",
    "    if i % ncols == 0:\n",
    "        ax.set_ylabel(\"count\")\n",
    "    if i >= (nrows - 1) * ncols:\n",
    "        ax.set_xlabel(\"burned fraction\")\n",
    "\n",
    "# Turn off unused slots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].axis(\"off\")\n",
    "\n",
    "# One legend for the whole figure\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc=\"upper right\")\n",
    "fig.suptitle(\"Burned fraction distributions across years (baseline vs thinning)\", y=1.02)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
