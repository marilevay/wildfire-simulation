{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2192889",
   "metadata": {},
   "source": [
    "# Main Wildfire Visualizations\n",
    "\n",
    "This notebook performs the end-to-end workflow for the wildfire simulation project:\n",
    "\n",
    "- Download Hansen Global Forest Change rasters for a single 10°×10° tile.\n",
    "- Crop + downsample the rasters into a tractable simulation grid.\n",
    "- Construct a simple annual fuel-density time series (2000–2019) from `treecover2000`, `lossyear`, `gain`, and `datamask`.\n",
    "- Run Monte Carlo wildfire simulations per year under two scenarios:\n",
    "  - **Baseline** (no mitigation)\n",
    "  - **Thinning** (uniform fuel reduction)\n",
    "- Visualize results (risk maps and time-series summaries).\n",
    "- Quantify thinning effectiveness with confidence intervals and a permutation-test p-value with a Holm–Bonferroni correction across years.\n",
    "\n",
    "Notes:\n",
    "- The selected tile is **60N_120W** (wilderness of northern Canada).\n",
    "- Many cells below can take time to run (especially Monte Carlo loops); keep `n_runs` modest while iterating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671f4297",
   "metadata": {},
   "source": [
    "# Imports\n",
    "\n",
    "These imports cover:\n",
    "- **Data acquisition**: `requests`, `PIL.Image`, `Path`\n",
    "- **Computation**: `numpy`\n",
    "- **Plotting**: `matplotlib`\n",
    "- **Progress bars**: `tqdm` for long-running loops\n",
    "\n",
    "Keeping imports near the top makes the notebook easier to rerun from a fresh kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2575c772",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import requests\n",
    "import io\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec6cba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the URL of the data tile\n",
    "\n",
    "# This project uses the Hansen GFC 2019 v1.7 dataset, which is distributed as 10°×10° GeoTIFF tiles.\n",
    "# The tile identifier is embedded in the filename (e.g., `60N_120W`).\n",
    "\n",
    "# You can swap this URL to analyze a different tile.\n",
    "url = (\n",
    "    \"https://storage.googleapis.com/earthenginepartners-hansen/GFC-2019-v1.7/\"\n",
    "    \"Hansen_GFC-2019-v1.7_treecover2000_60N_120W.tif\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a7840a",
   "metadata": {},
   "source": [
    "## Hansen Forest Tile Download\n",
    "\n",
    "We have to specify the URL of the data tile we want to download.\n",
    "The important part of the URL is the `00N_010E` part at the end.\n",
    "This specifies the latitude and longitude (in 10-degree units)\n",
    "of the data tile to download.\n",
    "\n",
    "The selected data tile is at `60N_120W`, or the wilderness of northern Canada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626ca3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data. These tiles are large, so we cache to disk and only convert the simulation window\n",
    "# to a NumPy array (instead of materializing the full 40,000 x 40,000 raster in memory).\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "def download_if_needed(url: str, out_path: str) -> str:\n",
    "    p = Path(out_path)\n",
    "    p.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if p.exists() and p.stat().st_size > 0:\n",
    "        return str(p)\n",
    "\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(p, \"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=1024 * 1024):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "    return str(p)\n",
    "\n",
    "\n",
    "def load_tif_window(path: str, row0: int, col0: int, window_px: int) -> np.ndarray:\n",
    "    im = Image.open(path)\n",
    "    box = (int(col0), int(row0), int(col0 + window_px), int(row0 + window_px))\n",
    "    return np.array(im.crop(box))\n",
    "\n",
    "\n",
    "treecover_path = download_if_needed(url, \"data/treecover2000_tile.tif\")\n",
    "\n",
    "# Use PIL to get raster dimensions without loading the full array.\n",
    "with Image.open(treecover_path) as _im:\n",
    "    tile_shape = (_im.height, _im.width)\n",
    "\n",
    "tile_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e5439b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: tile dimensions (should be 40000x40000)\n",
    "tile_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e79a789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a small part of the whole data tile (from disk-cached file).\n",
    "preview = load_tif_window(treecover_path, row0=2000, col0=1000, window_px=2000)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(preview, cmap='gray')\n",
    "plt.colorbar(shrink=0.8)\n",
    "plt.title(\"Raw Hansen treecover2000 (subset), values 0..100\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320b39e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the simulation package\n",
    "#\n",
    "# Specification:\n",
    "# - The `simulation/` package lives in this repository, so we add the repository root to `sys.path`\n",
    "#   to make `import simulation` work when running the notebook.\n",
    "\n",
    "# Note: If you install this project as a package instead, you can remove the sys.path manipulation.\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Make the project root importable so import simulation works\n",
    "sys.path.append(str(Path().resolve()))\n",
    "\n",
    "from simulation import FireModelParams, Forest, MonteCarlo, DataCollector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64245ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pool_2d(a: np.ndarray, out_shape: tuple[int, int]) -> np.ndarray:\n",
    "    a = np.asarray(a)\n",
    "    in_rows, in_cols = a.shape\n",
    "    out_rows, out_cols = out_shape\n",
    "\n",
    "    if in_rows % out_rows != 0 or in_cols % out_cols != 0:\n",
    "        raise ValueError(\"For this simple pooling method, crop so window dims are divisible by out dims\")\n",
    "\n",
    "    r = in_rows // out_rows\n",
    "    c = in_cols // out_cols\n",
    "    return a.reshape(out_rows, r, out_cols, c).mean(axis=(1, 3))\n",
    "\n",
    "\n",
    "# Select a window from the tile and downsample it to a simulation grid.\n",
    "row0, col0 = 2000, 1000\n",
    "window_px = 2000\n",
    "sim_shape = (200, 200)\n",
    "\n",
    "# Derive matching layer URLs from the treecover URL.\n",
    "tile_id = url.split(\"_treecover2000_\")[-1].replace(\".tif\", \"\")\n",
    "base = \"https://storage.googleapis.com/earthenginepartners-hansen/GFC-2019-v1.7/\"\n",
    "\n",
    "datamask_url = base + f\"Hansen_GFC-2019-v1.7_datamask_{tile_id}.tif\"\n",
    "lossyear_url = base + f\"Hansen_GFC-2019-v1.7_lossyear_{tile_id}.tif\"\n",
    "gain_url = base + f\"Hansen_GFC-2019-v1.7_gain_{tile_id}.tif\"\n",
    "\n",
    "datamask_path = download_if_needed(datamask_url, \"data/datamask_tile.tif\")\n",
    "lossyear_path = download_if_needed(lossyear_url, \"data/lossyear_tile.tif\")\n",
    "gain_path = download_if_needed(gain_url, \"data/gain_tile.tif\")\n",
    "\n",
    "# Load raw windows\n",
    "window_tree = load_tif_window(treecover_path, row0=row0, col0=col0, window_px=window_px)\n",
    "window_mask = load_tif_window(datamask_path, row0=row0, col0=col0, window_px=window_px)\n",
    "window_loss = load_tif_window(lossyear_path, row0=row0, col0=col0, window_px=window_px)\n",
    "window_gain = load_tif_window(gain_path, row0=row0, col0=col0, window_px=window_px)\n",
    "\n",
    "# Crop to be divisible for pooling\n",
    "h = sim_shape[0] * (window_tree.shape[0] // sim_shape[0])\n",
    "w = sim_shape[1] * (window_tree.shape[1] // sim_shape[1])\n",
    "window_tree = window_tree[:h, :w]\n",
    "window_mask = window_mask[:h, :w]\n",
    "window_loss = window_loss[:h, :w]\n",
    "window_gain = window_gain[:h, :w]\n",
    "\n",
    "# Downsample\n",
    "mask_small = mean_pool_2d(window_mask.astype(float), sim_shape)\n",
    "# datamask values are categorical; after pooling, interpret >1.5 as \"mostly water\" and <0.5 as \"mostly no data\".\n",
    "water_small = mask_small > 1.5\n",
    "nodata_small = mask_small < 0.5\n",
    "land_small = (~water_small) & (~nodata_small)\n",
    "\n",
    "base_density_2000 = np.clip(mean_pool_2d(window_tree.astype(float), sim_shape) / 100.0, 0.0, 1.0)\n",
    "base_density_2000 = base_density_2000 * land_small.astype(float)\n",
    "\n",
    "lossyear_small = mean_pool_2d(window_loss.astype(float), sim_shape)\n",
    "lossyear_small = np.rint(lossyear_small).astype(int)\n",
    "\n",
    "# gain is 0/1, but pooling yields fractions. Threshold at 0.5\n",
    "# (meaning >50% of the pooled pixels gained)\n",
    "gain_small = mean_pool_2d(window_gain.astype(float), sim_shape) > 0.5\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(base_density_2000, cmap=\"Greens\")\n",
    "plt.colorbar(shrink=0.8)\n",
    "plt.title(f\"Density (year 2000) with datamask applied {base_density_2000.shape}\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(water_small.astype(int), cmap=\"Blues\")\n",
    "plt.title(\"Water mask (1=water)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917a8765",
   "metadata": {},
   "source": [
    "# Risk Map time series \n",
    "From 2000 to 2019 using additional lossyear and gain masks \n",
    "\n",
    "**Assumptions:**\n",
    "- If a cell has loss in year Y (lossyear == Y-2000), then from that year onward the density is reduced.\n",
    "- gain is treated as a modest regrowth applied after 2012 in cells marked as gain.\n",
    "- water/no-data cells (datamask) always have density 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4cdb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a simple density time series (2000..2019) using lossyear and gain.\n",
    "\n",
    "p_burned, p_affected = [], []\n",
    "delta_burned, delta_affected = [], []\n",
    "\n",
    "def density_for_year(year: int) -> np.ndarray:\n",
    "    if year < 2000 or year > 2019:\n",
    "        raise ValueError(\"year must be in [2000, 2019]\")\n",
    "\n",
    "    d = base_density_2000.copy()\n",
    "\n",
    "    # Apply loss: lossyear values are 1..19 representing 2001..2019.\n",
    "    # For a given simulation year, treat any loss event up to that year as removing forest density.\n",
    "    cutoff = year - 2000\n",
    "    lost = (lossyear_small > 0) & (lossyear_small <= cutoff)\n",
    "\n",
    "    # Simple loss model: set to zero (stand-replacing disturbance)\n",
    "    d[lost] = 0.0\n",
    "\n",
    "    # Apply gain: only after 2012, modestly increase density in gain cells.\n",
    "    if year >= 2012:\n",
    "        d[gain_small] = np.clip(d[gain_small] + 0.25, 0.0, 1.0)\n",
    "\n",
    "    # Keep land mask enforced\n",
    "    d *= land_small.astype(float)\n",
    "    return d\n",
    "\n",
    "\n",
    "# Visualize density for a few years\n",
    "years_to_plot = (2000, 2003, 2005, 2008, 2010, 2012, 2015, 2017, 2019)\n",
    "for y in years_to_plot:\n",
    "    dy = density_for_year(y)\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(dy, cmap=\"Greens\")\n",
    "    plt.colorbar(shrink=0.8)\n",
    "    plt.title(f\"Density map (year={y})\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Monte Carlo over selected years + a strategy comparison (baseline vs thinning)\n",
    "params = FireModelParams(\n",
    "    wind=(1.0, 0.0),\n",
    "    wind_strength=0.4,\n",
    "    density_exponent=1.2,\n",
    "    base_spread=0.7,\n",
    ")\n",
    "\n",
    "n_runs = 200\n",
    "\n",
    "# Include more years to show intermediate changes.\n",
    "years_to_analyze = (2000, 2003, 2005, 2008, 2010, 2012, 2015, 2017, 2019)\n",
    "\n",
    "results = {}\n",
    "for y in tqdm(years_to_analyze, desc=\"Years\"):\n",
    "    d = density_for_year(y)\n",
    "    forest = Forest(d)\n",
    "    mc = MonteCarlo(forest=forest, params=params, n_runs=n_runs, rng=np.random.default_rng(y))\n",
    "\n",
    "    report = mc.run()\n",
    "    burned_arr, affected_arr = report.convert_to_arrays()\n",
    "    risk = mc.risk_map(n_runs=n_runs)\n",
    "    results[(y, \"baseline\")] = (burned_arr, affected_arr, risk)\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.hist(burned_arr, bins=25)\n",
    "    plt.title(f\"Year {y} baseline: burned fraction distribution\")\n",
    "    plt.xlabel(\"burned fraction\")\n",
    "    plt.ylabel(\"count\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    # Use a reversed colormap so higher risk appears darker\n",
    "    plt.imshow(risk, cmap=\"inferno_r\", vmin=0.0, vmax=1.0)\n",
    "    plt.colorbar(shrink=0.8)\n",
    "    plt.title(f\"Year {y} baseline: risk map\")\n",
    "    plt.show()\n",
    "\n",
    "    burned_mean, burned_lo, burned_hi = DataCollector.calculate_ci95_mean(burned_arr)\n",
    "    affected_mean, affected_lo, affected_hi = DataCollector.calculate_ci95_mean(affected_arr)\n",
    "    print(f\"Year {y} baseline burned mean+CI:\", (burned_mean, burned_lo, burned_hi))\n",
    "    print(f\"Year {y} baseline affected mean+CI:\", (affected_mean, affected_lo, affected_hi))\n",
    "\n",
    "    # Strategy: thinning\n",
    "    thin_factor = 0.6\n",
    "    forest_thin = forest.apply_thinning(thin_factor)\n",
    "    mc_thin = MonteCarlo(forest=forest_thin, params=params, n_runs=n_runs, rng=np.random.default_rng(y + 1))\n",
    "\n",
    "    report_thin = mc_thin.run()\n",
    "    burned_arr_t, affected_arr_t = report_thin.convert_to_arrays()\n",
    "    risk_thin = mc_thin.risk_map(n_runs=n_runs)\n",
    "    results[(y, \"thinning\")] = (burned_arr_t, affected_arr_t, risk_thin)\n",
    "\n",
    "    # Store p-values and effect sizes for later correction\n",
    "    # (define these lists once before the loop)\n",
    "    # p_burned = []; p_affected = []; delta_burned = []; delta_affected = []\n",
    "\n",
    "    p_b, d_b = permutation_p_value(burned_arr, burned_arr_t, n_perm=5000, alternative=\"greater\",\n",
    "                                rng=np.random.default_rng(1000 + y))\n",
    "    p_a, d_a = permutation_p_value(affected_arr, affected_arr_t, n_perm=5000, alternative=\"greater\",\n",
    "                                rng=np.random.default_rng(2000 + y))\n",
    "\n",
    "    p_burned.append(p_b); delta_burned.append(d_b)\n",
    "    p_affected.append(p_a); delta_affected.append(d_a)\n",
    "\n",
    "    print(f\"Year {y}: Delta burned={d_b:.4f}, p_raw_burned={p_b:.4g} | Delta affected={d_a:.4f}, p_raw_affected={p_a:.4g}\")\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.hist(burned_arr_t, bins=25)\n",
    "    plt.title(f\"Year {y} thinning(factor={thin_factor}): burned fraction\")\n",
    "    plt.xlabel(\"burned fraction\")\n",
    "    plt.ylabel(\"count\")\n",
    "    plt.show()\n",
    "\n",
    "    burned_mean_t, burned_lo_t, burned_hi_t = DataCollector.calculate_ci95_mean(burned_arr_t)\n",
    "    affected_mean_t, affected_lo_t, affected_hi_t = DataCollector.calculate_ci95_mean(affected_arr_t)\n",
    "    print(f\"Year {y} thinning burned mean+CI:\", (burned_mean_t, burned_lo_t, burned_hi_t))\n",
    "    print(f\"Year {y} thinning affected mean+CI:\", (affected_mean_t, affected_lo_t, affected_hi_t))\n",
    "\n",
    "\n",
    "# Summary line plot over time\n",
    "baseline_means = []\n",
    "thinning_means = []\n",
    "for y in years_to_analyze:\n",
    "    burned_b, _, _ = results[(y, \"baseline\")]\n",
    "    burned_t, _, _ = results[(y, \"thinning\")]\n",
    "    baseline_means.append(float(np.mean(burned_b)))\n",
    "    thinning_means.append(float(np.mean(burned_t)))\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(years_to_analyze, baseline_means, marker=\"o\", label=\"baseline\")\n",
    "plt.plot(years_to_analyze, thinning_means, marker=\"o\", label=\"thinning\")\n",
    "plt.title(\"Mean burned fraction over time (selected years)\")\n",
    "plt.xlabel(\"year\")\n",
    "plt.ylabel(\"mean burned fraction\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a229649",
   "metadata": {},
   "source": [
    "## Baseline vs Thinning Analysis\n",
    "We compare a baseline (no mitigation) scenario to a simple mitigation strategy:\n",
    "\n",
    "- **Thinning (fuel reduction):** multiply the forest density everywhere by a constant factor `c < 1`\n",
    "  (here `thin_factor = 0.6`).\n",
    "\n",
    "Lower density reduces spread probability and can disconnect fuel pathways, reducing expected burned\n",
    "and affected fractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ae146e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permutation test helpers (p-values) + Holm–Bonferroni correction\n",
    "#\n",
    "# We quantify thinning effectiveness with a nonparametric permutation test:\n",
    "# - Statistic: difference in means, T = mean(baseline) - mean(thinning)\n",
    "# - Alternative: thinning reduces outcomes, so we test H1: E[baseline] > E[thinning]\n",
    "#\n",
    "# We then correct across multiple years using Holm–Bonferroni to control family-wise error.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def permutation_p_value(x, y, n_perm=5000, alternative=\"greater\", rng=None):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng(0)\n",
    "\n",
    "    t_obs = x.mean() - y.mean()\n",
    "    pooled = np.concatenate([x, y])\n",
    "    n_x = len(x)\n",
    "\n",
    "    count = 0\n",
    "    for _ in range(int(n_perm)):\n",
    "        perm = rng.permutation(pooled)\n",
    "        x_p = perm[:n_x]\n",
    "        y_p = perm[n_x:]\n",
    "        t = x_p.mean() - y_p.mean()\n",
    "\n",
    "        if alternative == \"greater\":\n",
    "            count += (t >= t_obs)\n",
    "        elif alternative == \"less\":\n",
    "            count += (t <= t_obs)\n",
    "        else:\n",
    "            count += (abs(t) >= abs(t_obs))\n",
    "\n",
    "    return (count + 1) / (n_perm + 1), float(t_obs)\n",
    "\n",
    "\n",
    "def holm_bonferroni(p_values, alpha=0.05):\n",
    "    \"\"\"Holm–Bonferroni multiple comparisons correction.\n",
    "\n",
    "    Returns:\n",
    "      - adjusted_p: Holm adjusted p-values (same order as input)\n",
    "      - reject: whether each hypothesis is rejected at family-wise error rate alpha\n",
    "    \"\"\"\n",
    "    p = np.asarray(p_values, dtype=float)\n",
    "    m = len(p)\n",
    "    order = np.argsort(p)\n",
    "    p_sorted = p[order]\n",
    "\n",
    "    # Holm adjusted p-values: adj_i = max_{j<=i} ((m-j)*p_(j)) with j 0-indexed\n",
    "    adj_sorted = np.empty(m, dtype=float)\n",
    "    running_max = 0.0\n",
    "    for i, p_i in enumerate(p_sorted):\n",
    "        adj_i = (m - i) * p_i\n",
    "        running_max = max(running_max, adj_i)\n",
    "        adj_sorted[i] = min(1.0, running_max)\n",
    "\n",
    "    adjusted_p = np.empty(m, dtype=float)\n",
    "    adjusted_p[order] = adj_sorted\n",
    "\n",
    "    # Rejection decisions (step-down)\n",
    "    reject_sorted = np.zeros(m, dtype=bool)\n",
    "    for i, p_i in enumerate(p_sorted):\n",
    "        if p_i <= alpha / (m - i):\n",
    "            reject_sorted[i] = True\n",
    "        else:\n",
    "            break\n",
    "    reject = np.empty(m, dtype=bool)\n",
    "    reject[order] = reject_sorted\n",
    "\n",
    "    return adjusted_p, reject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b957d643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risk map summaries over time (convert 2D risk maps into time-series lines)\n",
    "#\n",
    "# A risk map is a 2D field of per-cell probabilities. To plot \"risk\" as a line over time, we reduce\n",
    "# each map to scalar summaries:\n",
    "# - mean risk across the grid\n",
    "# - upper-tail quantiles (p90, p99) to track hotspot intensity\n",
    "# - fraction of cells above a threshold (e.g., risk > 0.5) to track high-risk area\n",
    "\n",
    "years = list(years_to_analyze)\n",
    "\n",
    "def risk_summaries(risk, threshold=0.5):\n",
    "    r = np.asarray(risk, float).ravel()\n",
    "    return {\n",
    "        \"mean\": float(r.mean()),\n",
    "        \"p90\": float(np.quantile(r, 0.90)),\n",
    "        \"p99\": float(np.quantile(r, 0.99)),\n",
    "        \"frac_gt_t\": float((r > threshold).mean()),\n",
    "    }\n",
    "\n",
    "baseline_risk_mean = []\n",
    "thinning_risk_mean = []\n",
    "baseline_risk_p90 = []\n",
    "thinning_risk_p90 = []\n",
    "baseline_risk_frac = []\n",
    "thinning_risk_frac = []\n",
    "\n",
    "thr = 0.5\n",
    "\n",
    "for y in years:\n",
    "    _, _, risk_b = results[(y, \"baseline\")]\n",
    "    _, _, risk_t = results[(y, \"thinning\")]\n",
    "\n",
    "    sb = risk_summaries(risk_b, threshold=thr)\n",
    "    st = risk_summaries(risk_t, threshold=thr)\n",
    "\n",
    "    baseline_risk_mean.append(sb[\"mean\"])\n",
    "    thinning_risk_mean.append(st[\"mean\"])\n",
    "    baseline_risk_p90.append(sb[\"p90\"])\n",
    "    thinning_risk_p90.append(st[\"p90\"])\n",
    "    baseline_risk_frac.append(sb[\"frac_gt_t\"])\n",
    "    thinning_risk_frac.append(st[\"frac_gt_t\"])\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(years, baseline_risk_mean, marker=\"o\", label=\"baseline\")\n",
    "plt.plot(years, thinning_risk_mean, marker=\"o\", label=\"thinning\")\n",
    "plt.title(\"Mean risk over time\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Mean Risk\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(years, baseline_risk_p90, marker=\"o\", label=\"baseline\")\n",
    "plt.plot(years, thinning_risk_p90, marker=\"o\", label=\"thinning\")\n",
    "plt.title(\"90th percentile risk over time (hotspot intensity)\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Risk (90th percentile)\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(years, baseline_risk_frac, marker=\"o\", label=\"baseline\")\n",
    "plt.plot(years, thinning_risk_frac, marker=\"o\", label=\"thinning\")\n",
    "plt.title(f\"High-risk area over time (risk > {thr})\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Fraction of Cells\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2ce15b",
   "metadata": {},
   "source": [
    "## Burned and Affected Area Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46559f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Burned + affected fractions over time (mean ± 95% CI)\n",
    "#\n",
    "# This is the main time-series result:\n",
    "# - Burned fraction: fraction of cells that burn per run\n",
    "# - Affected fraction: burned + adjacent-to-burned per run\n",
    "#\n",
    "# For each year and scenario (baseline vs thinning), we compute a mean and a normal-approximation 95% CI.\n",
    "\n",
    "years = list(years_to_analyze)\n",
    "\n",
    "def mean_ci(arr):\n",
    "    m, lo, hi = DataCollector.calculate_ci95_mean(np.asarray(arr, float))\n",
    "    return float(m), float(lo), float(hi)\n",
    "\n",
    "burn_mean_b, burn_lo_b, burn_hi_b = [], [], []\n",
    "burn_mean_t, burn_lo_t, burn_hi_t = [], [], []\n",
    "aff_mean_b, aff_lo_b, aff_hi_b = [], [], []\n",
    "aff_mean_t, aff_lo_t, aff_hi_t = [], [], []\n",
    "\n",
    "for y in years:\n",
    "    burned_b, affected_b, _ = results[(y, \"baseline\")]\n",
    "    burned_t, affected_t, _ = results[(y, \"thinning\")]\n",
    "\n",
    "    m, lo, hi = mean_ci(burned_b)\n",
    "    burn_mean_b.append(m); burn_lo_b.append(lo); burn_hi_b.append(hi)\n",
    "\n",
    "    m, lo, hi = mean_ci(burned_t)\n",
    "    burn_mean_t.append(m); burn_lo_t.append(lo); burn_hi_t.append(hi)\n",
    "\n",
    "    m, lo, hi = mean_ci(affected_b)\n",
    "    aff_mean_b.append(m); aff_lo_b.append(lo); aff_hi_b.append(hi)\n",
    "\n",
    "    m, lo, hi = mean_ci(affected_t)\n",
    "    aff_mean_t.append(m); aff_lo_t.append(lo); aff_hi_t.append(hi)\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(years, burn_mean_b, marker=\"o\", label=\"baseline\")\n",
    "plt.fill_between(years, burn_lo_b, burn_hi_b, alpha=0.2)\n",
    "plt.plot(years, burn_mean_t, marker=\"o\", label=\"thinning\")\n",
    "plt.fill_between(years, burn_lo_t, burn_hi_t, alpha=0.2)\n",
    "plt.title(\"Burned fraction over time (mean ± 95% CI)\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Burned Fraction\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(years, aff_mean_b, marker=\"o\", label=\"baseline\")\n",
    "plt.fill_between(years, aff_lo_b, aff_hi_b, alpha=0.2)\n",
    "plt.plot(years, aff_mean_t, marker=\"o\", label=\"thinning\")\n",
    "plt.fill_between(years, aff_lo_t, aff_hi_t, alpha=0.2)\n",
    "plt.title(\"Affected fraction over time (mean ± 95% CI)\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Affected Fraction\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f4d6fd",
   "metadata": {},
   "source": [
    "## Significance Test of Thinning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dfc366",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "\n",
    "p_b_adj, rej_b = holm_bonferroni(p_burned, alpha=alpha)\n",
    "p_a_adj, rej_a = holm_bonferroni(p_affected, alpha=alpha)\n",
    "\n",
    "print(\"\\nMultiple comparisons correction (Holm–Bonferroni, per-metric across years)\\n\")\n",
    "for i, y in enumerate(years_to_analyze):\n",
    "    print(\n",
    "        f\"Year {y}: \"\n",
    "        f\"Delta burned={delta_burned[i]:.4f}, p_adj_burned={p_b_adj[i]:.4g}, reject={bool(rej_b[i])} | \"\n",
    "        f\"Delta affected={delta_affected[i]:.4f}, p_adj_affected={p_a_adj[i]:.4g}, reject={bool(rej_a[i])}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9960e67",
   "metadata": {},
   "source": [
    "# Theoretical vs empirical comparison (density scaling sweeps)\n",
    "This section connects the simulation to percolation/connectivity intuition.\n",
    "\n",
    "Idea:\n",
    "- Scale a fixed year's density map by a factor c in [0, 1] (uniform thinning).\n",
    "- Estimate the expected burned fraction as a function of c.\n",
    "- A nonlinear/transition-like curve supports the connectivity intuition.\n",
    "\n",
    "Minimal robustness setup:\n",
    "- Run the sweep for 3 representative years:\n",
    "  * 2000 (baseline)\n",
    "  * 2012 (gain breakpoint in our simplified time series)\n",
    "  * 2019 (end of available lossyear range)\n",
    "  \n",
    "Note: This is not part of the thinning significance test; it's a qualitative/theory check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179dd472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Density scaling sweep (multi-year overlay)\n",
    "\n",
    "base_years_for_sweep = (2000, 2012, 2019)\n",
    "\n",
    "# Keep modest so runtime stays reasonable.\n",
    "sweep_runs = 100\n",
    "scales = np.linspace(0.1, 1.0, 10)\n",
    "\n",
    "\n",
    "def run_density_scaling_sweep(base_year: int, scales, sweep_runs: int):\n",
    "    \"\"\"Run a uniform thinning sweep on a fixed base-year landscape.\n",
    "\n",
    "    Returns:\n",
    "      means: mean burned fraction at each scale\n",
    "      ci_lo/ci_hi: 95% CI bounds for the mean at each scale\n",
    "    \"\"\"\n",
    "    base_density = density_for_year(base_year)\n",
    "    base_forest = Forest(base_density)\n",
    "\n",
    "    means, ci_lo, ci_hi = [], [], []\n",
    "\n",
    "    for c in scales:\n",
    "        forest_c = base_forest.apply_thinning(float(c))\n",
    "        mc_c = MonteCarlo(\n",
    "            forest=forest_c,\n",
    "            params=params,\n",
    "            n_runs=sweep_runs,\n",
    "            rng=np.random.default_rng(base_year * 10_000 + int(1_000 * c)),\n",
    "        )\n",
    "\n",
    "        report_c = mc_c.run()\n",
    "        burned_c, _ = report_c.convert_to_arrays()\n",
    "        mean_c, lo_c, hi_c = DataCollector.calculate_ci95_mean(burned_c)\n",
    "\n",
    "        means.append(float(mean_c))\n",
    "        ci_lo.append(float(lo_c))\n",
    "        ci_hi.append(float(hi_c))\n",
    "\n",
    "    return means, ci_lo, ci_hi\n",
    "\n",
    "\n",
    "sweep_results = {y: run_density_scaling_sweep(y, scales=scales, sweep_runs=sweep_runs) for y in base_years_for_sweep}\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "for y in base_years_for_sweep:\n",
    "    means, lo, hi = sweep_results[y]\n",
    "    plt.plot(scales, means, marker=\"o\", label=f\"year {y}\")\n",
    "    plt.fill_between(scales, lo, hi, alpha=0.15)\n",
    "\n",
    "plt.title(\"Density scaling sweep (uniform thinning) across representative years\")\n",
    "plt.xlabel(\"Global density scale c (thinning factor)\")\n",
    "plt.ylabel(\"Burned fraction\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
